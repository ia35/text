// Copyright 2022 TF.Text Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "tensorflow_text/core/kernels/phrase_tokenizer.h"

#include <algorithm>
#include <iostream>
#include <ostream>
#include <string>
#include <vector>

#include "base/logging.h"
#include "absl/strings/str_join.h"
#include "absl/strings/string_view.h"
#include "icu4c/source/common/unicode/umachine.h"
#include "icu4c/source/common/unicode/utf8.h"
#include "tensorflow/lite/kernels/shim/status_macros.h"
#include "tensorflow_lite_support/custom_ops/kernel/sentencepiece/utils.h"
#include "tensorflow_text/core/kernels/whitespace_tokenizer_config_builder.h"

namespace tensorflow {
namespace text {

/*static*/ absl::StatusOr<PhraseTokenizer> PhraseTokenizer::Create(
    const void* config_flatbuffer) {
  PhraseTokenizer tokenizer;
  // `GetPhraseTokenizerConfig()` is autogenerated by flatbuffer.
  tokenizer.phrase_config_ = GetPhraseTokenizerConfig(config_flatbuffer);
  tokenizer.trie_ = absl::make_unique<sentencepiece::DoubleArrayTrie>(
      tokenizer.phrase_config_->vocab_trie()->nodes());
  tokenizer.prob_ = static_cast<float>(tokenizer.phrase_config_->prob()) / 100;
  tokenizer.whitespace_config_str_ = BuildWhitespaceTokenizerConfig();
  tokenizer.whitespace_config_ = absl::make_unique<WhitespaceTokenizerConfig>(
      tokenizer.whitespace_config_str_);
  return std::move(tokenizer);
}

void PhraseTokenizer::Tokenize(const absl::string_view input,
                               std::vector<std::string>* result_tokens,
                               std::vector<int>* result_token_ids) {
  const int input_size = input.size();
  int position = 0, prev_position = 0;
  UChar32 codepoint;
  bool inside_token = false;

  // Word level information.
  std::vector<std::string> tokens;
  std::vector<int> start_offsets;
  std::vector<int> end_offsets;

  while (position < input_size) {
    prev_position = position;
    U8_NEXT(input, position, input_size, codepoint);
    if (whitespace_config_->IsWhitespace(codepoint)) {
      if (inside_token) {
        int end_pos = position - 1;
        end_offsets.push_back(end_pos);
        int start_pos = start_offsets.back();
        std::string token(input.substr(start_pos, end_pos - start_pos));
        tokens.push_back(token);
        inside_token = false;
      }
    } else {
      if (!inside_token) {
        start_offsets.push_back(prev_position);
        inside_token = true;
      }
    }
  }
  // save final word
  if (inside_token) {
    int end_pos = position;
    end_offsets.push_back(end_pos);
    int start_pos = start_offsets.back();
    std::string token(input.substr(start_pos, end_pos - start_pos));
    tokens.push_back(token);
  }

  auto concat = [](std::string a, std::string b) {
    if (a.empty()) {
      return b;
    }
    if (b.empty()) {
      return a;
    }
    return std::move(a) + ' ' + std::move(b);
  };
  std::string all_str =
      std::accumulate(tokens.begin(), tokens.end(), std::string(), concat);

  FindPhraseTokens(all_str, result_tokens, result_token_ids);
}

void PhraseTokenizer::FindPhraseTokens(const std::string& cur_phrase,
                                       std::vector<std::string>* phrase_tokens,
                                       std::vector<int>* phrase_token_ids) {
  // Do a simple left to right.
  int index = 0;
  while (index < cur_phrase.size()) {
    bool in_vocab = false;
    int token_id = phrase_config_->unk_token_id();
    int length = 0;
    PhraseLookup(cur_phrase, index, &in_vocab, &token_id, &length);
    if (!in_vocab) {
      // fall back to using single token.
      std::size_t found = cur_phrase.find_first_of(' ', index);
      phrase_tokens->push_back("<UNK>");
      phrase_token_ids->push_back(phrase_config_->unk_token_id());
      if (found == std::string::npos) {
        break;
      }
      index = found + 1;
    } else {
      // find a phrase.
      phrase_tokens->push_back(cur_phrase.substr(index, length));
      phrase_token_ids->push_back(token_id);
      index += (length + 1);
    }
  }
}

void PhraseTokenizer::PhraseLookup(const std::string& token, int cur,
                                   bool* in_vocab, int* index, int* length) {
  int prev_word_index = -1;
  int prev_word_length = 0;
  bool has_output = false;
  float prob = prob_;
  absl::BitGen* gen = &gen_;
  auto token_emit = [&token, cur, prob, in_vocab, index, length,
                     &prev_word_index, &prev_word_length, &has_output,
                     gen](const sentencepiece::DoubleArrayTrie::Match& m) {
    if (has_output || (cur + m.match_length < token.size() &&
                       token[cur + m.match_length] != ' ')) {
      return;
    }

    prev_word_index = m.id;
    prev_word_length = m.match_length;
    *in_vocab = true;
    bool coin_flip = absl::Bernoulli(*gen, prob);
    if (coin_flip) {
      // Note that we emit shorter phrase first.
      *index = m.id;
      *length = m.match_length;
      has_output = true;
    }
  };
  trie_->IteratePrefixMatches(
      sentencepiece::utils::string_view(token.data() + cur, token.size() - cur),
      token_emit);
  if (*in_vocab && !has_output) {
    // We should use prev longest one as output as we prefer longer ones.
    *index = prev_word_index;
    *length = prev_word_length;
  }
}

absl::StatusOr<std::vector<std::string>> PhraseTokenizer::DetokenizeToTokens(
    const absl::Span<const int> input) const {
  std::vector<std::string> output_tokens;
  if (!phrase_config_->support_detokenization()) {
    return absl::FailedPreconditionError(
        "Detokenize function is only enabled when support_detokenization is "
        "true in the config flatbuffer. Please rebuild the model flatbuffer "
        "by setting support_detokenization=true.");
  }
  for (int id : input) {
    auto vocab = phrase_config_->vocab_array()->Get(id);
    output_tokens.emplace_back(vocab->string_view());
  }
  return output_tokens;
}

absl::StatusOr<std::string> PhraseTokenizer::Detokenize(
    const absl::Span<const int> input) const {
  SH_ASSIGN_OR_RETURN(std::vector<std::string> output_tokens,
                      DetokenizeToTokens(input));
  return absl::StrJoin(output_tokens, " ");
}

}  // namespace text
}  // namespace tensorflow
